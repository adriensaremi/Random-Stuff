{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_dataset.ipynb","provenance":[],"authorship_tag":"ABX9TyOJjDXxyf9McrTplfQDlpc3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RbAtki7mjupS"},"source":["# **The MNIST Dataset: Building a Deep Neural Network Classifier from scratch**\n","\n","For more information, check out the following link:\n","\n","https://www.youtube.com/watch?v=HMcx-zY8JSg&t=548s&ab_channel=HvassLaboratories"]},{"cell_type":"code","metadata":{"id":"wBxnW9LEsMra","executionInfo":{"status":"ok","timestamp":1605049824169,"user_tz":300,"elapsed":2017,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}},"outputId":"f266af67-c021-4f96-be37-97d39943617a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.compat.v1 as tf\n","import math\n","\n","tf.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.3.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"YbjRv4D_kNKU"},"source":["### Importing the dataset"]},{"cell_type":"code","metadata":{"id":"wa-VCvL1tg0D","executionInfo":{"status":"ok","timestamp":1605049825683,"user_tz":300,"elapsed":900,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}},"outputId":"aa1f2029-f4b0-4e78-a283-9c74137d4f85","colab":{"base_uri":"https://localhost:8080/"}},"source":["data = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n","print('Data is set as tuple of numpy arrays: (x_train, y_train), (x_test, y_test)')\n","(x_train, y_train), (x_test, y_test) = data\n","print('- With training-set:')\n","print('x_train has dimensions {} corresponding to # of images the images\\'shape' .format(x_train.shape))\n","print('y_train has dimensions {} corresponding to the labels'.format(y_train.shape[0]))\n","print('- With testing-set:')\n","print('x_test has dimensions {} corresponding to # of images the images\\'shape' .format(x_test.shape))\n","print('y_test has dimensions {} corresponding to the labels'.format(y_test.shape[0]))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Data is set as tuple of numpy arrays: (x_train, y_train), (x_test, y_test)\n","- With training-set:\n","x_train has dimensions (60000, 28, 28) corresponding to # of images the images'shape\n","y_train has dimensions 60000 corresponding to the labels\n","- With testing-set:\n","x_test has dimensions (10000, 28, 28) corresponding to # of images the images'shape\n","y_test has dimensions 10000 corresponding to the labels\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZlVbHEv34N3L","executionInfo":{"status":"ok","timestamp":1605049850577,"user_tz":300,"elapsed":357,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}},"outputId":"092cf725-2b95-47a6-d53b-1fc1ae5c3787","colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["pixels = data[0][0][0]\n","plt.imshow(pixels.astype('uint8'), cmap='binary')\n","plt.xticks([])\n","plt.yticks([])\n","plt.show()\n","print('Target value:', data[0][1][0])"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Target value: 5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qy21l2p-JtQX"},"source":["Let's get a couple parameters about our data set:"]},{"cell_type":"code","metadata":{"id":"QUYfN6Fj2vuh","executionInfo":{"status":"ok","timestamp":1605058344432,"user_tz":300,"elapsed":249,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["#shape of images (i.e. number of pixels)\n","img_shape = np.shape(data[0][0][0])\n","\n","#channel per pixels (1 channel for grey-scale)\n","num_channels = 1\n","\n","#number of classes: the digits running from 0 to 9\n","num_classes = 10"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9L3INrn-2oJV"},"source":["# **Preparing up the Model**\n","\n","- placeholders\n","- helper functions\n","- the architecture of the neural network, the weights and the ouputs\n","- optimizing the cost-function\n"]},{"cell_type":"markdown","metadata":{"id":"-rMNX-C8mWR_"},"source":["### **Placeholders**\n","\n","We define placeholder variables; these include the input and the ouput of the computational graph in TensorFlow. \n","- We start with the input images x, which we convert to a 4-dim array x_image, expected for the inputs of each layer.\n","- We also account for the output y_true, a 1D array of 10 entries, each corresponding to a class. Finally y_true_clas which returns the argmax of y_true"]},{"cell_type":"code","metadata":{"id":"4F1hIe4UiiK8","executionInfo":{"status":"ok","timestamp":1605051998106,"user_tz":300,"elapsed":6632,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["# Version 2.3.0 of tf comes with eager execution enabled by default. We must disable it before proceeding\n","tf.disable_eager_execution()\n","x = tf.placeholder(tf.float32, shape=[None, img_shape[0]*img_shape[1]], name='x')\n","x_image = tf.reshape(x, [-1, img_shape[0], img_shape[1], num_channels])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5-9AQNFh8cu","executionInfo":{"status":"ok","timestamp":1605052026441,"user_tz":300,"elapsed":224,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n","y_true_cls = tf.argmax(y_true, axis=1)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPOt2jS9nPwG"},"source":["### **Helper-Functions**\n","\n","These allow us to create new variables and to define the shape of the **convulational neural network**. Importantly enough, we define for future use functions for:\n","- the weights and the biases of the network\n","- the convolutional layers\n","- the fully-connected layers"]},{"cell_type":"code","metadata":{"id":"1Ht1buE4ktaG","executionInfo":{"status":"ok","timestamp":1605061375237,"user_tz":300,"elapsed":241,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["def new_weights(shape):\n","  return tf.Variable(tf.truncated_normal(shape, mean = 0,  stddev=0.5))\n","\n","def new_biases(length):\n","  return tf.Variable(tf.constant(0.05, shape=[length]))"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUTLcQFKpc3J"},"source":["Each **convolutional layer** is defined by the **input** images and the **filters**. Specifically, each layer inputs a 4-dimensional tensor described by:\n","- the number of images\n","- dimension 1 of each image\n","- dimension 2 of each image\n","- the number of channels\n","\n","And the filters are described:\n","- the number of filters\n","- the shape of the filters"]},{"cell_type":"code","metadata":{"id":"pqydim_3rDYO","executionInfo":{"status":"ok","timestamp":1605061404929,"user_tz":300,"elapsed":229,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["def conv_layer(input,               # Outputs from the previous layer: must account for num. of channels as well\n","               filters,             # The filters of the layer: must account num. of filters and their shape\n","               use_pooling=True):   # Use 2x2 max-pooling to reduce image resolution\n","    \n","    num_input_channels = input.shape[-1]\n","    filter_size = filters[0]        # the shape of the filters, assuming it is square\n","    num_filters = filters[1]        # the number of filters\n","\n","    # Create new weights and new biases from the filters-weights for the convolution.\n","    # The weights are stored in a 4D array corresponding the shape of the filters, the number of channels and the number of filters\n","    weights = new_weights(shape=[filter_size, filter_size, num_input_channels, num_filters])\n","    # # The biases correspond only to filters\n","    biases = new_biases(length=num_filters)\n","\n","    # The layer, with the biases. Strides indicate we move 1 image, 1 pixel (down and right) and 1 channel at a time\n","    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n","    layer += biases\n","\n","    # If use_pooling true, let's down-sample the image resolution. ksize indicates which max value to take\n","    if use_pooling:\n","        layer = tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    # Rectified Linear Unit (ReLU).\n","    layer = tf.nn.relu(layer)\n","\n","    return layer, weights"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q84LCEe02iBh","executionInfo":{"status":"ok","timestamp":1605060589867,"user_tz":300,"elapsed":256,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["def flatten_layer(layer):\n","\n","    layer_shape = layer.get_shape()\n","    # The shape of the input layer is assumed to be:\n","    # layer_shape == [num_images, img_height, img_width, num_channels]\n","    # The number of features is: img_height * img_width * num_channels\n","    num_features = layer_shape[1:4].num_elements()\n","\n","    # The shape of the flattened layer is now:\n","    # [num_images, img_height * img_width * num_channels]\n","    layer_flat = tf.reshape(layer, [-1, num_features])\n","\n","    return layer_flat"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpirfHLZJJGX"},"source":["A **fully connected layer** only needs the input images and the number of outputs. Remember that for a fully connected layer $L$, there are $N_L$ neurons:\n","\n","$\\mathbf{x}^{L} = \\mathbf{W}^L \\mathbf{x}^{L-1} + \\mathbf{b}^L$\n","and $\\mathbf{x}^{L} = h(\\mathbf{x}^{L})$ where:\n","- $\\mathbf{x}^{L}$ in the space of $\\mathbb{R}^{N_L}$\n","- $\\mathbf{W}^L$ in the space of $\\mathbb{R}^{N_L} \\times \\mathbb{R}^{N_L - 1}$\n","- $h$ is the activation function (ReLU for example)\n","\n","For a **CNN**, the relationship above holds but the elements $\\mathbf{x}^{L}$ and $\\mathbf{W}^{L}$ are *4-dim tensors*."]},{"cell_type":"code","metadata":{"id":"ScNxXolJ3xNJ","executionInfo":{"status":"ok","timestamp":1605060627026,"user_tz":300,"elapsed":297,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["def fc_layer(input,          # Outputs from the previous layer, must account for num. of inputs\n","             num_outputs,    # Num. of outputs\n","             use_relu=True): # Use Rectified Linear Unit by default\n","\n","    num_inputs = input.shape[-1]\n","\n","    # Create new weights and biases\n","    weights = new_weights(shape=[num_inputs, num_outputs])\n","    biases = new_biases(length=num_outputs)\n","\n","    # Calculate the new layer using simple matrix mulitplication\n","    layer = tf.matmul(input, weights) + biases\n","    if use_relu:\n","        layer = tf.nn.relu(layer)\n","\n","    return layer"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBJIwwZS4mxe"},"source":["### **The Neural Network Architecture**\n","\n","It is made of two sequential convolutional layers, followed by two sequential fully-connected layers. A flatten layer is needed between the CNN's and the FC's. Let's use the following parameters:"]},{"cell_type":"code","metadata":{"id":"DU3xZ88B8MEj","executionInfo":{"status":"ok","timestamp":1605058943935,"user_tz":300,"elapsed":189,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["# Convolutional Layer 1.\n","filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n","num_filters1 = 16         # There are 16 of these filters.\n","\n","# Convolutional Layer 2.\n","filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n","num_filters2 = 36         # There are 36 of these filters.\n","filters2 = (filter_size2, num_filters2)\n","\n","# Fully-connected layer.\n","fc_size = 128             # Number of neurons in fully-connected layer."],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"9s9-Lvfr5iY3","executionInfo":{"status":"ok","timestamp":1605061772386,"user_tz":300,"elapsed":215,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["layer_conv1, weights_conv1 = conv_layer(x_image, (filter_size1, num_filters1), use_pooling=True)\n","\n","layer_conv2, weights_conv2 = conv_layer(layer_conv1, (filter_size2, num_filters2), use_pooling=True)\n","\n","layer_flat = flatten_layer(layer_conv2)\n","\n","layer_fc1 = fc_layer(layer_flat, fc_size, use_relu=True)\n","\n","layer_fc2 = fc_layer(layer_fc1, num_classes, use_relu=False)"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8pgnevfb5I8a"},"source":["And **the results**"]},{"cell_type":"code","metadata":{"id":"9FV1f0liJPoz","executionInfo":{"status":"ok","timestamp":1605062049594,"user_tz":300,"elapsed":212,"user":{"displayName":"Adrien Saremi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0ZSAV18xEymPdyqWMJJ-pnKkV_oWwtPginHIxBQ=s64","userId":"05689223645961180094"}}},"source":["y_pred = tf.nn.softmax(layer_fc2)\n","\n","y_pred_class = tf.argmax(y_pred, axis = 1)"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CkMfzWJEKUSH"},"source":["### **Cost-function Optimization**"]},{"cell_type":"markdown","metadata":{"id":"gnDGgfCrkTlg"},"source":["# **Running the Model**\n","\n","fff"]}]}